{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "My_Tweets_Scrapper.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCBy-RSJVSI5",
        "outputId": "de97d07a-3a26-4c4e-ccb2-c70ca4096b40"
      },
      "source": [
        "!apt update\n",
        "!apt install chromium-chromedriver\n",
        "!pip install selenium\n",
        "!pip install BeautifulSoup4\n",
        "\n",
        "from selenium import webdriver\n",
        "\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "\u001b[33m\r0% [Waiting for headers] [Connecting to security.ubuntu.com (91.189.91.38)] [Co\u001b[0m\r                                                                               \rHit:2 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "\u001b[33m\r0% [Connecting to security.ubuntu.com (91.189.91.38)] [Connected to cloud.r-pro\u001b[0m\r                                                                               \rGet:3 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "\r                                                                               \rHit:4 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "\u001b[33m\r0% [3 InRelease 47.5 kB/88.7 kB 54%] [Connecting to security.ubuntu.com (91.189\u001b[0m\u001b[33m\r0% [2 InRelease gpgv 242 kB] [3 InRelease 47.5 kB/88.7 kB 54%] [Connecting to s\u001b[0m\u001b[33m\r0% [2 InRelease gpgv 242 kB] [Waiting for headers] [Connecting to security.ubun\u001b[0m\r                                                                               \rGet:5 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "\u001b[33m\r0% [2 InRelease gpgv 242 kB] [Waiting for headers] [Connecting to security.ubun\u001b[0m\r                                                                               \rGet:6 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "\u001b[33m\r0% [2 InRelease gpgv 242 kB] [6 InRelease 14.2 kB/74.6 kB 19%] [Connecting to s\u001b[0m\r                                                                               \rHit:7 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:8 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [15.9 kB]\n",
            "Ign:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:10 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Ign:11 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "Hit:13 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:14 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:15 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,759 kB]\n",
            "Get:16 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [900 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [31.6 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,546 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [426 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,181 kB]\n",
            "Get:21 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [53.9 kB]\n",
            "Get:22 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [53.2 kB]\n",
            "Ign:24 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
            "Get:24 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [742 kB]\n",
            "Get:25 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,116 kB]\n",
            "Get:26 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [24.7 kB]\n",
            "Get:27 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,410 kB]\n",
            "Get:28 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [396 kB]\n",
            "Fetched 12.9 MB in 3s (4,267 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "69 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 69 not upgraded.\n",
            "Need to get 86.6 MB of archives.\n",
            "After this operation, 300 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 90.0.4430.72-0ubuntu0.18.04.1 [1,128 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 90.0.4430.72-0ubuntu0.18.04.1 [76.9 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 90.0.4430.72-0ubuntu0.18.04.1 [3,858 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 90.0.4430.72-0ubuntu0.18.04.1 [4,743 kB]\n",
            "Fetched 86.6 MB in 1s (65.5 MB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 160690 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_90.0.4430.72-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (90.0.4430.72-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_90.0.4430.72-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-browser (90.0.4430.72-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_90.0.4430.72-0ubuntu0.18.04.1_all.deb ...\n",
            "Unpacking chromium-browser-l10n (90.0.4430.72-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_90.0.4430.72-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (90.0.4430.72-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (90.0.4430.72-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser (90.0.4430.72-0ubuntu0.18.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (90.0.4430.72-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser-l10n (90.0.4430.72-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Collecting selenium\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB)\n",
            "\u001b[K     |████████████████████████████████| 911kB 13.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from selenium) (1.24.3)\n",
            "Installing collected packages: selenium\n",
            "Successfully installed selenium-3.141.0\n",
            "Requirement already satisfied: BeautifulSoup4 in /usr/local/lib/python3.7/dist-packages (4.6.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "usid = input('Enter your Twitter Username:')\n",
        "pswd = input('Enter your Twitter Password:')"
      ],
      "metadata": {
        "id": "nDXiXh5Ncb2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wYA3w4MURhm",
        "outputId": "21142773-fefa-4da6-dcd3-0edd3811271b"
      },
      "source": [
        "target = input('Enter the Twitter User handle you wish to scrap : ')\n",
        "maxNum = int(input('Enter the number of tweets to scrape : '))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter the Twitter account name : the rock\n",
            "Enter the minimum number of tweets to scrape : 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6E51Wh9Wf2y"
      },
      "source": [
        "driver = webdriver.Chrome('chromedriver')\n",
        "driver.get('https://twitter.com/login')\n",
        "time.sleep(2)\n",
        "\n",
        "\n",
        "login = driver.find_element_by_xpath('//*[@id=\"react-root\"]/div/div/div[2]/main/div/div/div[2]/form/div/div[1]/label/div/div[2]/div/input')\n",
        "login.send_keys(usid)\n",
        "\n",
        "password = driver.find_element_by_xpath('//*[@id=\"react-root\"]/div/div/div[2]/main/div/div/div[2]/form/div/div[2]/label/div/div[2]/div/input')\n",
        "password.send_keys(pswd)\n",
        "\n",
        "login_button = driver.find_element_by_xpath('//*[@id=\"react-root\"]/div/div/div[2]/main/div/div/div[2]/form/div/div[3]/div/div')\n",
        "login_button.click()\n",
        "\n",
        "WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"react-root\"]/div/div/div[2]/main/div/div/div/div[2]/div/div[2]/div/div/div/div[1]/div/div/div/form/div[1]/div/div/div[2]/input')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqzvaS1gWyjW"
      },
      "source": [
        "def return_duplicate(lst):\n",
        "    oc_set = set() \n",
        "    res = [] \n",
        "    for idx, val in enumerate(lst): \n",
        "        if val not in oc_set: \n",
        "            oc_set.add(val)          \n",
        "        else: \n",
        "            res.append(idx)\n",
        "    return res+++++++++++"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QD6_0CekWmoV"
      },
      "source": [
        "search = driver.find_element_by_xpath('//*[@id=\"react-root\"]/div/div/div[2]/main/div/div/div/div[2]/div/div[2]/div/div/div/div[1]/div/div/div/form/div[1]/div/div/div[2]/input')\n",
        "search.send_keys(target)\n",
        "search.send_keys(Keys.ENTER)\n",
        "time.sleep(1)\n",
        "\n",
        "ppl = driver.find_element_by_xpath('//*[@id=\"react-root\"]/div/div/div[2]/main/div/div/div/div[1]/div/div[1]/div[2]/nav/div/div[2]/div/div[3]/a')\n",
        "ppl.click()\n",
        "time.sleep(2)-*/iv[2]/div/div/section/div/div/div[1]/div/div/div/div[2]/div[1]/div[1]/a/div/div[1]/div[1]/span/span')\n",
        "profile.click()\n",
        "time.sleep(1)\n",
        "\n",
        "tweets = list()\n",
        "time_stamps = list()\n",
        "soup = BeautifulSoup(driver.page_source, 'lxml')\n",
        "postings = soup.find_all('div', class_ = 'css-901oao r-1fmj7o5 r-1qd0xha r-a023e6 r-16dba41 r-ad9z0x r-bcqeeo r-bnwqim r-qvutc0')\n",
        "timings = soup.find_all('a', class_ = 'css-4rbku5 css-18t94o4 css-901oao r-9ilb82 r-1loqt21 r-1q142lx r-1qd0xha r-a023e6 r-16dba41 r-ad9z0x r-bcqeeo r-3s2u2q r-qvutc0')\n",
        "\n",
        "\n",
        "dup_idx = sorted(return_duplicate(postings), reverse = True)\n",
        "for ele in dup_idx:  \n",
        "    del postings[ele]\n",
        "for ele in dup_idx:  \n",
        "    del timings[ele]\n",
        "\n",
        "    \n",
        "while len(tweets) < maxNum:\n",
        "    for post in postings:\n",
        "        tweets.append(post.text)\n",
        "    for t in timings:\n",
        "        time_stamps.append(t.text)\n",
        "    print(str(len(time_stamps)) + '   ' + str(len(tweets)))\n",
        "    #tweets = list(set(tweets))\n",
        "    driver.execute_script('window.scrollTo(0, document.body.scrollHeight)')\n",
        "    time.sleep(1)\n",
        "    soup = BeautifulSoup(driver.page_source, 'lxml')\n",
        "    postings = soup.find_all('div', class_ = 'css-901oao r-1fmj7o5 r-1qd0xha r-a023e6 r-16dba41 r-ad9z0x r-bcqeeo r-bnwqim r-qvutc0')\n",
        "    timings = soup.find_all('a', class_ = 'css-4rbku5 css-18t94o4 css-901oao r-9ilb82 r-1loqt21 r-1q142lx r-1qd0xha r-a023e6 r-16dba41 r-ad9z0x r-bcqeeo r-3s2u2q r-qvutc0')\n",
        "\n",
        "    dup_idx = sorted(return_duplicate(postings), reverse = True)\n",
        "    for ele in dup_idx:  \n",
        "        del postings[ele]\n",
        "    for ele in dup_idx:  \n",
        "        del timings[ele]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iud7WvurW5R6"
      },
      "source": [
        "table = pd.DataFrame({'Time Stamp' : time_stamps, 'Tweets' : tweets})\n",
        "table.to_csv('sp_out.csv', header=False, index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}